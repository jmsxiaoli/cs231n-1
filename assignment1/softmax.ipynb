{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.366243\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "$f(x_i; W) =  W x_i$\n",
    "\n",
    "Since initial $W_j \\approx 0$, it follows that $f_j \\approx 0$, therefore\n",
    "\n",
    "$$L = -\\log \\frac{e^{f_{y_i}}}{\\sum_{j=0}^{C-1} e^{f_j}} \\approx -\\log \\frac{e^0}{\\sum_{j=0}^{C-1} e^0} = -\\log \\frac{1}{C} = \\log C$$\n",
    "\n",
    "where $C$ is the total number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.350448 analytic: -0.350448, relative error: 2.657268e-08\n",
      "numerical: -1.216147 analytic: -1.216147, relative error: 4.376003e-09\n",
      "numerical: -0.637600 analytic: -0.637600, relative error: 3.991602e-08\n",
      "numerical: 2.641045 analytic: 2.641045, relative error: 1.929742e-08\n",
      "numerical: 3.391305 analytic: 3.391305, relative error: 2.617591e-10\n",
      "numerical: -2.073814 analytic: -2.073815, relative error: 3.222983e-08\n",
      "numerical: 1.678430 analytic: 1.678430, relative error: 1.139013e-08\n",
      "numerical: 2.064429 analytic: 2.064429, relative error: 1.768319e-08\n",
      "numerical: 0.356027 analytic: 0.356027, relative error: 5.854007e-08\n",
      "numerical: -2.126551 analytic: -2.126551, relative error: 6.443563e-10\n",
      "numerical: 0.092976 analytic: 0.092976, relative error: 1.476376e-07\n",
      "numerical: -0.403939 analytic: -0.403939, relative error: 1.067359e-07\n",
      "numerical: -3.313148 analytic: -3.313148, relative error: 1.402173e-08\n",
      "numerical: 0.367220 analytic: 0.367220, relative error: 1.920413e-08\n",
      "numerical: -2.536370 analytic: -2.536370, relative error: 8.145157e-10\n",
      "numerical: -0.669562 analytic: -0.669562, relative error: 4.982045e-08\n",
      "numerical: 4.209506 analytic: 4.209506, relative error: 6.497293e-09\n",
      "numerical: 4.358752 analytic: 4.358752, relative error: 2.144047e-09\n",
      "numerical: -0.801802 analytic: -0.801802, relative error: 3.384074e-08\n",
      "numerical: -3.211408 analytic: -3.211409, relative error: 1.783515e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.366243e+00 computed in 0.099999s\n",
      "vectorized loss: 2.366243e+00 computed in 0.010017s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 21.366167 learning rate 0.000000 regularization 500.000000\n",
      "iteration 100 / 1500: loss 20.775926 learning rate 0.000000 regularization 500.000000\n",
      "iteration 200 / 1500: loss 20.343800 learning rate 0.000000 regularization 500.000000\n",
      "iteration 300 / 1500: loss 19.750855 learning rate 0.000000 regularization 500.000000\n",
      "iteration 400 / 1500: loss 19.676940 learning rate 0.000000 regularization 500.000000\n",
      "iteration 500 / 1500: loss 19.710108 learning rate 0.000000 regularization 500.000000\n",
      "iteration 600 / 1500: loss 19.694890 learning rate 0.000000 regularization 500.000000\n",
      "iteration 700 / 1500: loss 19.076489 learning rate 0.000000 regularization 500.000000\n",
      "iteration 800 / 1500: loss 18.704285 learning rate 0.000000 regularization 500.000000\n",
      "iteration 900 / 1500: loss 19.022028 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1000 / 1500: loss 18.915992 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1100 / 1500: loss 18.395645 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1200 / 1500: loss 18.673262 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1300 / 1500: loss 18.576110 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1400 / 1500: loss 18.563873 learning rate 0.000000 regularization 500.000000\n",
      "iteration 0 / 1500: loss 36.555193 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 100 / 1500: loss 35.901466 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 200 / 1500: loss 35.775526 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 300 / 1500: loss 35.996113 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 400 / 1500: loss 35.190631 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 500 / 1500: loss 35.561338 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 600 / 1500: loss 34.656826 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 700 / 1500: loss 34.247693 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 800 / 1500: loss 34.201233 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 900 / 1500: loss 34.145360 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1000 / 1500: loss 33.653822 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1100 / 1500: loss 33.511080 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1200 / 1500: loss 33.381124 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1300 / 1500: loss 33.371994 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1400 / 1500: loss 33.459707 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 0 / 1500: loss 156.945049 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 100 / 1500: loss 153.674959 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 200 / 1500: loss 150.286134 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 300 / 1500: loss 147.156762 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 400 / 1500: loss 143.956205 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 500 / 1500: loss 140.869921 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 600 / 1500: loss 137.981906 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 700 / 1500: loss 135.192528 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 800 / 1500: loss 132.648465 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 900 / 1500: loss 129.878785 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1000 / 1500: loss 127.276689 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1100 / 1500: loss 124.649544 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1200 / 1500: loss 122.343379 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1300 / 1500: loss 119.510076 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1400 / 1500: loss 117.304839 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 0 / 1500: loss 309.321036 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 100 / 1500: loss 296.871547 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 200 / 1500: loss 284.937685 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 300 / 1500: loss 273.709121 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 400 / 1500: loss 262.539011 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 500 / 1500: loss 252.313777 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 600 / 1500: loss 242.145126 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 700 / 1500: loss 232.783141 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 800 / 1500: loss 223.412205 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 900 / 1500: loss 214.765586 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1000 / 1500: loss 206.306096 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1100 / 1500: loss 198.223226 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1200 / 1500: loss 190.337535 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1300 / 1500: loss 183.104996 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1400 / 1500: loss 176.050237 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 0 / 1500: loss 1524.413039 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 100 / 1500: loss 1247.845749 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 200 / 1500: loss 1021.537983 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 300 / 1500: loss 836.754097 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 400 / 1500: loss 685.126774 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 500 / 1500: loss 560.985602 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 600 / 1500: loss 459.420966 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 700 / 1500: loss 376.510117 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 800 / 1500: loss 308.389976 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 900 / 1500: loss 252.823286 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1000 / 1500: loss 207.442044 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1100 / 1500: loss 170.062267 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1200 / 1500: loss 139.538065 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1300 / 1500: loss 114.570586 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1400 / 1500: loss 94.186369 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 0 / 1500: loss 3030.976080 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 100 / 1500: loss 2031.339750 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 200 / 1500: loss 1360.969807 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 300 / 1500: loss 912.303293 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 400 / 1500: loss 611.703063 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 500 / 1500: loss 410.230210 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 600 / 1500: loss 275.599465 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 700 / 1500: loss 185.388744 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 800 / 1500: loss 124.827829 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 900 / 1500: loss 84.345732 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1000 / 1500: loss 57.252451 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1100 / 1500: loss 39.076764 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1200 / 1500: loss 26.860255 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1300 / 1500: loss 18.732052 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1400 / 1500: loss 13.246264 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 0 / 1500: loss 15271.308676 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 100 / 1500: loss 2047.348788 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 200 / 1500: loss 276.107799 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 300 / 1500: loss 38.923837 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 400 / 1500: loss 7.173955 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 500 / 1500: loss 2.931063 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 600 / 1500: loss 2.359797 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 700 / 1500: loss 2.286539 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 800 / 1500: loss 2.268150 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 900 / 1500: loss 2.273640 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1000 / 1500: loss 2.265454 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1100 / 1500: loss 2.272751 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1200 / 1500: loss 2.269092 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1300 / 1500: loss 2.274006 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1400 / 1500: loss 2.266789 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 0 / 1500: loss 20.233169 learning rate 0.000000 regularization 500.000000\n",
      "iteration 100 / 1500: loss 19.360102 learning rate 0.000000 regularization 500.000000\n",
      "iteration 200 / 1500: loss 18.748297 learning rate 0.000000 regularization 500.000000\n",
      "iteration 300 / 1500: loss 18.055427 learning rate 0.000000 regularization 500.000000\n",
      "iteration 400 / 1500: loss 18.307056 learning rate 0.000000 regularization 500.000000\n",
      "iteration 500 / 1500: loss 17.596898 learning rate 0.000000 regularization 500.000000\n",
      "iteration 600 / 1500: loss 17.977052 learning rate 0.000000 regularization 500.000000\n",
      "iteration 700 / 1500: loss 17.069578 learning rate 0.000000 regularization 500.000000\n",
      "iteration 800 / 1500: loss 16.896452 learning rate 0.000000 regularization 500.000000\n",
      "iteration 900 / 1500: loss 16.651633 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1000 / 1500: loss 16.566872 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1100 / 1500: loss 16.465166 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1200 / 1500: loss 16.024881 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1300 / 1500: loss 15.758088 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1400 / 1500: loss 15.782715 learning rate 0.000000 regularization 500.000000\n",
      "iteration 0 / 1500: loss 35.879751 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 100 / 1500: loss 33.764309 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 200 / 1500: loss 32.936433 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 300 / 1500: loss 31.999078 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 400 / 1500: loss 31.650067 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 500 / 1500: loss 31.100858 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 600 / 1500: loss 30.023130 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 700 / 1500: loss 29.670801 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 800 / 1500: loss 28.905978 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 900 / 1500: loss 28.279518 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1000 / 1500: loss 27.596404 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1100 / 1500: loss 27.006792 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1200 / 1500: loss 26.719938 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1300 / 1500: loss 26.026489 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1400 / 1500: loss 25.436787 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 0 / 1500: loss 157.900450 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 100 / 1500: loss 141.536732 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 200 / 1500: loss 127.939767 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 300 / 1500: loss 115.299648 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 400 / 1500: loss 104.413067 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 500 / 1500: loss 94.524814 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 600 / 1500: loss 85.530196 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 700 / 1500: loss 77.543646 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 800 / 1500: loss 70.215767 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 900 / 1500: loss 63.653306 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1000 / 1500: loss 57.802888 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1100 / 1500: loss 52.280581 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1200 / 1500: loss 47.592183 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1300 / 1500: loss 43.177225 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1400 / 1500: loss 39.233865 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 0 / 1500: loss 315.176159 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 100 / 1500: loss 257.320361 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 200 / 1500: loss 210.845294 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 300 / 1500: loss 172.348845 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 400 / 1500: loss 141.374728 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 500 / 1500: loss 116.005130 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 600 / 1500: loss 95.246553 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 700 / 1500: loss 78.246503 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 800 / 1500: loss 64.297638 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 900 / 1500: loss 52.966172 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1000 / 1500: loss 43.705109 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1100 / 1500: loss 36.060166 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1200 / 1500: loss 29.803590 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1300 / 1500: loss 24.863427 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1400 / 1500: loss 20.603829 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 0 / 1500: loss 1517.316558 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 100 / 1500: loss 557.097461 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 200 / 1500: loss 205.413906 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 300 / 1500: loss 76.611785 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 400 / 1500: loss 29.404619 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 500 / 1500: loss 12.082951 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 600 / 1500: loss 5.772838 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 700 / 1500: loss 3.476356 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 800 / 1500: loss 2.631383 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 900 / 1500: loss 2.289934 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1000 / 1500: loss 2.200433 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1100 / 1500: loss 2.192010 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1200 / 1500: loss 2.160451 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1300 / 1500: loss 2.159977 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1400 / 1500: loss 2.125042 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 0 / 1500: loss 3084.776176 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 100 / 1500: loss 414.413348 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 200 / 1500: loss 57.321696 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 300 / 1500: loss 9.581755 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 400 / 1500: loss 3.186465 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 500 / 1500: loss 2.276145 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 600 / 1500: loss 2.207670 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 700 / 1500: loss 2.166113 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 800 / 1500: loss 2.186854 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 900 / 1500: loss 2.185473 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1000 / 1500: loss 2.183445 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1100 / 1500: loss 2.173970 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1200 / 1500: loss 2.250003 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1300 / 1500: loss 2.194030 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1400 / 1500: loss 2.197274 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 0 / 1500: loss 15239.443064 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 100 / 1500: loss 2.797536 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 200 / 1500: loss 2.269796 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 300 / 1500: loss 2.249662 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 400 / 1500: loss 2.271297 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 500 / 1500: loss 2.268865 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 600 / 1500: loss 2.266053 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 700 / 1500: loss 2.257937 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 800 / 1500: loss 2.268264 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 900 / 1500: loss 2.280108 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1000 / 1500: loss 2.275951 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1100 / 1500: loss 2.268397 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1200 / 1500: loss 2.267836 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1300 / 1500: loss 2.267239 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1400 / 1500: loss 2.270051 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 0 / 1500: loss 20.829921 learning rate 0.000000 regularization 500.000000\n",
      "iteration 100 / 1500: loss 18.724553 learning rate 0.000000 regularization 500.000000\n",
      "iteration 200 / 1500: loss 18.038609 learning rate 0.000000 regularization 500.000000\n",
      "iteration 300 / 1500: loss 17.433886 learning rate 0.000000 regularization 500.000000\n",
      "iteration 400 / 1500: loss 17.174801 learning rate 0.000000 regularization 500.000000\n",
      "iteration 500 / 1500: loss 17.121496 learning rate 0.000000 regularization 500.000000\n",
      "iteration 600 / 1500: loss 16.408840 learning rate 0.000000 regularization 500.000000\n",
      "iteration 700 / 1500: loss 15.888749 learning rate 0.000000 regularization 500.000000\n",
      "iteration 800 / 1500: loss 15.494622 learning rate 0.000000 regularization 500.000000\n",
      "iteration 900 / 1500: loss 15.343694 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1000 / 1500: loss 14.966446 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1100 / 1500: loss 14.835082 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1200 / 1500: loss 14.265100 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1300 / 1500: loss 13.813363 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1400 / 1500: loss 13.900283 learning rate 0.000000 regularization 500.000000\n",
      "iteration 0 / 1500: loss 36.009228 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 100 / 1500: loss 33.256053 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 200 / 1500: loss 31.447063 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 300 / 1500: loss 29.876675 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 400 / 1500: loss 28.956009 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 500 / 1500: loss 27.526628 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 600 / 1500: loss 26.333485 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 700 / 1500: loss 25.456411 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 800 / 1500: loss 24.367432 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 900 / 1500: loss 23.568456 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1000 / 1500: loss 22.738700 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1100 / 1500: loss 21.819862 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1200 / 1500: loss 21.001102 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1300 / 1500: loss 20.126092 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1400 / 1500: loss 19.213800 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 0 / 1500: loss 158.744077 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 100 / 1500: loss 128.969067 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 200 / 1500: loss 105.625141 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 300 / 1500: loss 86.590304 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 400 / 1500: loss 70.848834 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 500 / 1500: loss 58.108989 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 600 / 1500: loss 47.993349 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 700 / 1500: loss 39.488486 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 800 / 1500: loss 32.616945 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 900 / 1500: loss 27.033564 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1000 / 1500: loss 22.551074 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1100 / 1500: loss 18.660667 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1200 / 1500: loss 15.604950 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1300 / 1500: loss 13.128163 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1400 / 1500: loss 11.127890 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 0 / 1500: loss 311.128614 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 100 / 1500: loss 208.051845 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 200 / 1500: loss 139.628450 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 300 / 1500: loss 93.868565 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 400 / 1500: loss 63.421266 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 500 / 1500: loss 43.130276 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 600 / 1500: loss 29.512295 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 700 / 1500: loss 20.292723 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 800 / 1500: loss 14.305392 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 900 / 1500: loss 10.159835 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1000 / 1500: loss 7.481938 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1100 / 1500: loss 5.656172 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1200 / 1500: loss 4.458865 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1300 / 1500: loss 3.613053 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1400 / 1500: loss 3.163015 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 0 / 1500: loss 1520.006653 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 100 / 1500: loss 204.568839 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 200 / 1500: loss 29.117901 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 300 / 1500: loss 5.783838 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 400 / 1500: loss 2.596279 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 500 / 1500: loss 2.200091 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 600 / 1500: loss 2.121729 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 700 / 1500: loss 2.118160 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 800 / 1500: loss 2.132188 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 900 / 1500: loss 2.176458 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1000 / 1500: loss 2.184641 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1100 / 1500: loss 2.100264 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1200 / 1500: loss 2.146715 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1300 / 1500: loss 2.177804 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1400 / 1500: loss 2.161718 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 0 / 1500: loss 3058.314992 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 100 / 1500: loss 55.685786 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 200 / 1500: loss 3.104443 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 300 / 1500: loss 2.236530 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 400 / 1500: loss 2.184044 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 500 / 1500: loss 2.188064 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 600 / 1500: loss 2.163965 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 700 / 1500: loss 2.177904 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 800 / 1500: loss 2.219426 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 900 / 1500: loss 2.179836 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1000 / 1500: loss 2.220535 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1100 / 1500: loss 2.221004 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1200 / 1500: loss 2.209918 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1300 / 1500: loss 2.220903 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1400 / 1500: loss 2.151033 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 0 / 1500: loss 15448.226684 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 100 / 1500: loss 2.263435 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 200 / 1500: loss 2.272944 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 300 / 1500: loss 2.265321 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 400 / 1500: loss 2.252447 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 500 / 1500: loss 2.276288 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 600 / 1500: loss 2.270703 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 700 / 1500: loss 2.273497 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 800 / 1500: loss 2.267375 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 900 / 1500: loss 2.258729 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1000 / 1500: loss 2.274468 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1100 / 1500: loss 2.265741 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1200 / 1500: loss 2.260083 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1300 / 1500: loss 2.278191 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1400 / 1500: loss 2.276445 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 0 / 1500: loss 21.174986 learning rate 0.000000 regularization 500.000000\n",
      "iteration 100 / 1500: loss 16.863180 learning rate 0.000000 regularization 500.000000\n",
      "iteration 200 / 1500: loss 15.280806 learning rate 0.000000 regularization 500.000000\n",
      "iteration 300 / 1500: loss 13.549119 learning rate 0.000000 regularization 500.000000\n",
      "iteration 400 / 1500: loss 12.211436 learning rate 0.000000 regularization 500.000000\n",
      "iteration 500 / 1500: loss 11.371647 learning rate 0.000000 regularization 500.000000\n",
      "iteration 600 / 1500: loss 10.414133 learning rate 0.000000 regularization 500.000000\n",
      "iteration 700 / 1500: loss 9.587709 learning rate 0.000000 regularization 500.000000\n",
      "iteration 800 / 1500: loss 8.625194 learning rate 0.000000 regularization 500.000000\n",
      "iteration 900 / 1500: loss 8.168411 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1000 / 1500: loss 7.340287 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1100 / 1500: loss 6.867445 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1200 / 1500: loss 6.408205 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1300 / 1500: loss 5.832160 learning rate 0.000000 regularization 500.000000\n",
      "iteration 1400 / 1500: loss 5.374173 learning rate 0.000000 regularization 500.000000\n",
      "iteration 0 / 1500: loss 36.500526 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 100 / 1500: loss 27.721709 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 200 / 1500: loss 22.538714 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 300 / 1500: loss 18.552161 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 400 / 1500: loss 15.590831 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 500 / 1500: loss 12.975687 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 600 / 1500: loss 10.929972 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 700 / 1500: loss 9.218662 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 800 / 1500: loss 7.905824 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 900 / 1500: loss 6.725047 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1000 / 1500: loss 5.878548 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1100 / 1500: loss 5.001374 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1200 / 1500: loss 4.540092 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1300 / 1500: loss 4.096647 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 1400 / 1500: loss 3.642959 learning rate 0.000000 regularization 1000.000000\n",
      "iteration 0 / 1500: loss 156.883108 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 100 / 1500: loss 57.662142 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 200 / 1500: loss 22.305598 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 300 / 1500: loss 9.307979 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 400 / 1500: loss 4.577038 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 500 / 1500: loss 2.947240 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 600 / 1500: loss 2.294703 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 700 / 1500: loss 2.110226 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 800 / 1500: loss 1.956903 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 900 / 1500: loss 1.967924 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1000 / 1500: loss 2.028643 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1100 / 1500: loss 1.910408 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1200 / 1500: loss 1.915468 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1300 / 1500: loss 1.910737 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 1400 / 1500: loss 1.872263 learning rate 0.000000 regularization 5000.000000\n",
      "iteration 0 / 1500: loss 312.350792 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 100 / 1500: loss 42.951363 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 200 / 1500: loss 7.448852 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 300 / 1500: loss 2.750867 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 400 / 1500: loss 2.063579 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 500 / 1500: loss 1.996648 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 600 / 1500: loss 2.013850 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 700 / 1500: loss 1.970025 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 800 / 1500: loss 2.012568 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 900 / 1500: loss 2.054000 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1000 / 1500: loss 1.944841 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1100 / 1500: loss 2.053116 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1200 / 1500: loss 1.986925 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1300 / 1500: loss 1.967442 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 1400 / 1500: loss 2.003334 learning rate 0.000000 regularization 10000.000000\n",
      "iteration 0 / 1500: loss 1551.774824 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 100 / 1500: loss 2.268352 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 200 / 1500: loss 2.126830 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 300 / 1500: loss 2.143633 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 400 / 1500: loss 2.111779 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 500 / 1500: loss 2.159368 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 600 / 1500: loss 2.115973 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 700 / 1500: loss 2.162618 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 800 / 1500: loss 2.158377 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 900 / 1500: loss 2.121505 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1000 / 1500: loss 2.162754 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1100 / 1500: loss 2.139951 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1200 / 1500: loss 2.145108 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1300 / 1500: loss 2.176020 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 1400 / 1500: loss 2.153219 learning rate 0.000000 regularization 50000.000000\n",
      "iteration 0 / 1500: loss 3047.301375 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 100 / 1500: loss 2.226805 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 200 / 1500: loss 2.206064 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 300 / 1500: loss 2.201962 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 400 / 1500: loss 2.197872 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 500 / 1500: loss 2.236409 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 600 / 1500: loss 2.162672 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 700 / 1500: loss 2.227880 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 800 / 1500: loss 2.203947 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 900 / 1500: loss 2.216235 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1000 / 1500: loss 2.215945 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1100 / 1500: loss 2.193790 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1200 / 1500: loss 2.196578 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1300 / 1500: loss 2.250772 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 1400 / 1500: loss 2.145110 learning rate 0.000000 regularization 100000.000000\n",
      "iteration 0 / 1500: loss 15555.070974 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 100 / 1500: loss 2.280436 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 200 / 1500: loss 2.272683 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 300 / 1500: loss 2.272840 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 400 / 1500: loss 2.265505 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 500 / 1500: loss 2.280244 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 600 / 1500: loss 2.271751 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 700 / 1500: loss 2.276649 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 800 / 1500: loss 2.275135 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 900 / 1500: loss 2.289659 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1000 / 1500: loss 2.279771 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1100 / 1500: loss 2.265703 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1200 / 1500: loss 2.278385 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1300 / 1500: loss 2.267813 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 1400 / 1500: loss 2.288459 learning rate 0.000000 regularization 500000.000000\n",
      "iteration 0 / 1500: loss 22.365191 learning rate 0.000001 regularization 500.000000\n",
      "iteration 100 / 1500: loss 14.912658 learning rate 0.000001 regularization 500.000000\n",
      "iteration 200 / 1500: loss 12.466989 learning rate 0.000001 regularization 500.000000\n",
      "iteration 300 / 1500: loss 10.527801 learning rate 0.000001 regularization 500.000000\n",
      "iteration 400 / 1500: loss 8.810711 learning rate 0.000001 regularization 500.000000\n",
      "iteration 500 / 1500: loss 7.475054 learning rate 0.000001 regularization 500.000000\n",
      "iteration 600 / 1500: loss 6.405496 learning rate 0.000001 regularization 500.000000\n",
      "iteration 700 / 1500: loss 5.492883 learning rate 0.000001 regularization 500.000000\n",
      "iteration 800 / 1500: loss 4.661563 learning rate 0.000001 regularization 500.000000\n",
      "iteration 900 / 1500: loss 4.163874 learning rate 0.000001 regularization 500.000000\n",
      "iteration 1000 / 1500: loss 3.778687 learning rate 0.000001 regularization 500.000000\n",
      "iteration 1100 / 1500: loss 3.360393 learning rate 0.000001 regularization 500.000000\n",
      "iteration 1200 / 1500: loss 3.157093 learning rate 0.000001 regularization 500.000000\n",
      "iteration 1300 / 1500: loss 2.784751 learning rate 0.000001 regularization 500.000000\n",
      "iteration 1400 / 1500: loss 2.667734 learning rate 0.000001 regularization 500.000000\n",
      "iteration 0 / 1500: loss 35.299697 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 100 / 1500: loss 22.935644 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 200 / 1500: loss 15.780137 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 300 / 1500: loss 11.047719 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 400 / 1500: loss 7.961626 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 500 / 1500: loss 5.907958 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 600 / 1500: loss 4.613112 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 700 / 1500: loss 3.613547 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 800 / 1500: loss 3.071736 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 900 / 1500: loss 2.562905 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 1000 / 1500: loss 2.426063 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 1100 / 1500: loss 2.146217 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 1200 / 1500: loss 2.049085 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 1300 / 1500: loss 2.084254 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 1400 / 1500: loss 1.946397 learning rate 0.000001 regularization 1000.000000\n",
      "iteration 0 / 1500: loss 158.034866 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 100 / 1500: loss 22.146444 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 200 / 1500: loss 4.642501 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 300 / 1500: loss 2.306273 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 400 / 1500: loss 1.950185 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 500 / 1500: loss 1.975621 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 600 / 1500: loss 2.017484 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 700 / 1500: loss 1.957625 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 800 / 1500: loss 1.992243 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 900 / 1500: loss 2.058839 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 1000 / 1500: loss 1.970989 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 1100 / 1500: loss 1.944241 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 1200 / 1500: loss 1.993706 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 1300 / 1500: loss 1.921455 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 1400 / 1500: loss 2.021789 learning rate 0.000001 regularization 5000.000000\n",
      "iteration 0 / 1500: loss 313.505231 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 100 / 1500: loss 7.311715 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 200 / 1500: loss 2.098829 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 300 / 1500: loss 2.013094 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 400 / 1500: loss 2.033133 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 500 / 1500: loss 1.960918 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 600 / 1500: loss 2.099038 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 700 / 1500: loss 1.957017 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 800 / 1500: loss 2.066956 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 900 / 1500: loss 2.049058 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 1000 / 1500: loss 2.018944 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 1100 / 1500: loss 2.045689 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 1200 / 1500: loss 2.061634 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 1300 / 1500: loss 2.025679 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 1400 / 1500: loss 2.016496 learning rate 0.000001 regularization 10000.000000\n",
      "iteration 0 / 1500: loss 1561.646138 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 100 / 1500: loss 2.158013 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 200 / 1500: loss 2.129313 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 300 / 1500: loss 2.109825 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 400 / 1500: loss 2.172251 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 500 / 1500: loss 2.144564 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 600 / 1500: loss 2.166734 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 700 / 1500: loss 2.148175 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 800 / 1500: loss 2.150909 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 900 / 1500: loss 2.146909 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 1000 / 1500: loss 2.141356 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 1100 / 1500: loss 2.151269 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 1200 / 1500: loss 2.157364 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 1300 / 1500: loss 2.151937 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 1400 / 1500: loss 2.198222 learning rate 0.000001 regularization 50000.000000\n",
      "iteration 0 / 1500: loss 3109.222267 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 100 / 1500: loss 2.227574 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 200 / 1500: loss 2.194879 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 300 / 1500: loss 2.225439 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 400 / 1500: loss 2.171673 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 500 / 1500: loss 2.207629 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 600 / 1500: loss 2.236614 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 700 / 1500: loss 2.183614 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 800 / 1500: loss 2.233733 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 900 / 1500: loss 2.201793 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 1000 / 1500: loss 2.240819 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 1100 / 1500: loss 2.199370 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 1200 / 1500: loss 2.219191 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 1300 / 1500: loss 2.198257 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 1400 / 1500: loss 2.203206 learning rate 0.000001 regularization 100000.000000\n",
      "iteration 0 / 1500: loss 15623.575219 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 100 / 1500: loss 2.299797 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 200 / 1500: loss 2.294266 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 300 / 1500: loss 2.307621 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 400 / 1500: loss 2.322979 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 500 / 1500: loss 2.289660 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 600 / 1500: loss 2.284830 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 700 / 1500: loss 2.310223 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 800 / 1500: loss 2.293246 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 900 / 1500: loss 2.318703 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 1000 / 1500: loss 2.310165 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 1100 / 1500: loss 2.314620 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 1200 / 1500: loss 2.298188 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 1300 / 1500: loss 2.293746 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 1400 / 1500: loss 2.311186 learning rate 0.000001 regularization 500000.000000\n",
      "iteration 0 / 1500: loss 21.494974 learning rate 0.000005 regularization 500.000000\n",
      "iteration 100 / 1500: loss 7.569315 learning rate 0.000005 regularization 500.000000\n",
      "iteration 200 / 1500: loss 3.878202 learning rate 0.000005 regularization 500.000000\n",
      "iteration 300 / 1500: loss 2.653151 learning rate 0.000005 regularization 500.000000\n",
      "iteration 400 / 1500: loss 2.119996 learning rate 0.000005 regularization 500.000000\n",
      "iteration 500 / 1500: loss 1.918804 learning rate 0.000005 regularization 500.000000\n",
      "iteration 600 / 1500: loss 1.878482 learning rate 0.000005 regularization 500.000000\n",
      "iteration 700 / 1500: loss 1.913217 learning rate 0.000005 regularization 500.000000\n",
      "iteration 800 / 1500: loss 1.947415 learning rate 0.000005 regularization 500.000000\n",
      "iteration 900 / 1500: loss 1.852604 learning rate 0.000005 regularization 500.000000\n",
      "iteration 1000 / 1500: loss 1.840102 learning rate 0.000005 regularization 500.000000\n",
      "iteration 1100 / 1500: loss 1.781291 learning rate 0.000005 regularization 500.000000\n",
      "iteration 1200 / 1500: loss 2.013681 learning rate 0.000005 regularization 500.000000\n",
      "iteration 1300 / 1500: loss 1.925253 learning rate 0.000005 regularization 500.000000\n",
      "iteration 1400 / 1500: loss 1.923519 learning rate 0.000005 regularization 500.000000\n",
      "iteration 0 / 1500: loss 37.897875 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 100 / 1500: loss 6.044854 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 200 / 1500: loss 2.319233 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 300 / 1500: loss 2.042245 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 400 / 1500: loss 2.122070 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 500 / 1500: loss 2.049331 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 600 / 1500: loss 2.317516 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 700 / 1500: loss 1.975301 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 800 / 1500: loss 2.028886 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 900 / 1500: loss 2.122936 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 1000 / 1500: loss 1.879881 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 1100 / 1500: loss 1.936844 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 1200 / 1500: loss 1.904668 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 1300 / 1500: loss 2.103009 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 1400 / 1500: loss 2.015411 learning rate 0.000005 regularization 1000.000000\n",
      "iteration 0 / 1500: loss 161.493682 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 100 / 1500: loss 2.200036 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 200 / 1500: loss 2.118877 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 300 / 1500: loss 2.027031 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 400 / 1500: loss 2.163463 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 500 / 1500: loss 2.011192 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 600 / 1500: loss 2.286558 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 700 / 1500: loss 1.986689 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 800 / 1500: loss 2.253737 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 900 / 1500: loss 2.125436 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 1000 / 1500: loss 2.192080 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 1100 / 1500: loss 2.039250 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 1200 / 1500: loss 1.958393 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 1300 / 1500: loss 2.136803 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 1400 / 1500: loss 2.122304 learning rate 0.000005 regularization 5000.000000\n",
      "iteration 0 / 1500: loss 312.812623 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 100 / 1500: loss 2.120238 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 200 / 1500: loss 2.417740 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 300 / 1500: loss 2.280707 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 400 / 1500: loss 2.120732 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 500 / 1500: loss 2.135359 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 600 / 1500: loss 2.323597 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 700 / 1500: loss 2.147992 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 800 / 1500: loss 2.191642 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 900 / 1500: loss 2.036579 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 1000 / 1500: loss 2.214611 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 1100 / 1500: loss 2.255916 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 1200 / 1500: loss 2.308844 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 1300 / 1500: loss 2.107171 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 1400 / 1500: loss 2.253398 learning rate 0.000005 regularization 10000.000000\n",
      "iteration 0 / 1500: loss 1532.345213 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 100 / 1500: loss 3.522323 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 200 / 1500: loss 3.397999 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 300 / 1500: loss 3.665912 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 400 / 1500: loss 4.107840 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 500 / 1500: loss 3.730650 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 600 / 1500: loss 3.991624 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 700 / 1500: loss 3.453225 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 800 / 1500: loss 2.576727 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 900 / 1500: loss 3.708073 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 1000 / 1500: loss 4.090687 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 1100 / 1500: loss 3.693014 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 1200 / 1500: loss 3.566570 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 1300 / 1500: loss 4.755288 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 1400 / 1500: loss 4.247864 learning rate 0.000005 regularization 50000.000000\n",
      "iteration 0 / 1500: loss 3087.803183 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 100 / 1500: loss 6.872120 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 200 / 1500: loss 7.936830 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 300 / 1500: loss 8.559082 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 400 / 1500: loss 7.328547 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 500 / 1500: loss 8.074189 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 600 / 1500: loss 8.317825 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 700 / 1500: loss 8.821381 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 800 / 1500: loss 9.430613 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 900 / 1500: loss 6.599935 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 1000 / 1500: loss 9.755167 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 1100 / 1500: loss 8.903169 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 1200 / 1500: loss 9.692416 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 1300 / 1500: loss 7.834114 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 1400 / 1500: loss 9.182157 learning rate 0.000005 regularization 100000.000000\n",
      "iteration 0 / 1500: loss 15381.461577 learning rate 0.000005 regularization 500000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roman\\Repositories\\cs321n\\assignment1\\cs231n\\classifiers\\softmax.py:85: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= np.mean(np.log(target_scores))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss inf learning rate 0.000005 regularization 500000.000000\n",
      "iteration 200 / 1500: loss inf learning rate 0.000005 regularization 500000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roman\\Repositories\\cs321n\\assignment1\\cs231n\\classifiers\\softmax.py:91: RuntimeWarning: overflow encountered in double_scalars\n",
      "  loss += reg * np.sum(np.diagonal(W.T.dot(W)))\n",
      "C:\\Users\\Roman\\.venv\\cs231n\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1500: loss inf learning rate 0.000005 regularization 500000.000000\n",
      "iteration 400 / 1500: loss inf learning rate 0.000005 regularization 500000.000000\n",
      "iteration 500 / 1500: loss inf learning rate 0.000005 regularization 500000.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roman\\Repositories\\cs321n\\assignment1\\cs231n\\classifiers\\softmax.py:92: RuntimeWarning: overflow encountered in multiply\n",
      "  dW += reg*2*W\n",
      "C:\\Users\\Roman\\Repositories\\cs321n\\assignment1\\cs231n\\classifiers\\softmax.py:79: RuntimeWarning: invalid value encountered in subtract\n",
      "  scores -= np.max(scores, axis=1).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 700 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 800 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 900 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 1000 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 1100 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 1200 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 1300 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "iteration 1400 / 1500: loss nan learning rate 0.000005 regularization 500000.000000\n",
      "lr 1.000000e-08 reg 5.000000e+02 train accuracy: 0.155429 val accuracy: 0.148000\n",
      "lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.133429 val accuracy: 0.127000\n",
      "lr 1.000000e-08 reg 5.000000e+03 train accuracy: 0.157673 val accuracy: 0.152000\n",
      "lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.162510 val accuracy: 0.188000\n",
      "lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.208837 val accuracy: 0.225000\n",
      "lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.268184 val accuracy: 0.273000\n",
      "lr 1.000000e-08 reg 5.000000e+05 train accuracy: 0.261286 val accuracy: 0.269000\n",
      "lr 5.000000e-08 reg 5.000000e+02 train accuracy: 0.222367 val accuracy: 0.214000\n",
      "lr 5.000000e-08 reg 1.000000e+03 train accuracy: 0.226694 val accuracy: 0.246000\n",
      "lr 5.000000e-08 reg 5.000000e+03 train accuracy: 0.256551 val accuracy: 0.243000\n",
      "lr 5.000000e-08 reg 1.000000e+04 train accuracy: 0.296735 val accuracy: 0.306000\n",
      "lr 5.000000e-08 reg 5.000000e+04 train accuracy: 0.303612 val accuracy: 0.318000\n",
      "lr 5.000000e-08 reg 1.000000e+05 train accuracy: 0.284265 val accuracy: 0.296000\n",
      "lr 5.000000e-08 reg 5.000000e+05 train accuracy: 0.255408 val accuracy: 0.270000\n",
      "lr 1.000000e-07 reg 5.000000e+02 train accuracy: 0.263082 val accuracy: 0.258000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.270000 val accuracy: 0.294000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.332388 val accuracy: 0.344000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.349408 val accuracy: 0.371000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.303714 val accuracy: 0.319000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.297776 val accuracy: 0.309000\n",
      "lr 1.000000e-07 reg 5.000000e+05 train accuracy: 0.251510 val accuracy: 0.272000\n",
      "lr 5.000000e-07 reg 5.000000e+02 train accuracy: 0.364143 val accuracy: 0.365000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.387776 val accuracy: 0.386000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.371143 val accuracy: 0.376000\n",
      "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.360571 val accuracy: 0.367000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.309286 val accuracy: 0.334000\n",
      "lr 5.000000e-07 reg 1.000000e+05 train accuracy: 0.285224 val accuracy: 0.303000\n",
      "lr 5.000000e-07 reg 5.000000e+05 train accuracy: 0.233306 val accuracy: 0.234000\n",
      "lr 1.000000e-06 reg 5.000000e+02 train accuracy: 0.398592 val accuracy: 0.404000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.396367 val accuracy: 0.390000\n",
      "lr 1.000000e-06 reg 5.000000e+03 train accuracy: 0.368429 val accuracy: 0.382000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.357531 val accuracy: 0.357000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.296633 val accuracy: 0.302000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.289714 val accuracy: 0.297000\n",
      "lr 1.000000e-06 reg 5.000000e+05 train accuracy: 0.216939 val accuracy: 0.237000\n",
      "lr 5.000000e-06 reg 5.000000e+02 train accuracy: 0.348367 val accuracy: 0.328000\n",
      "lr 5.000000e-06 reg 1.000000e+03 train accuracy: 0.329653 val accuracy: 0.340000\n",
      "lr 5.000000e-06 reg 5.000000e+03 train accuracy: 0.304429 val accuracy: 0.318000\n",
      "lr 5.000000e-06 reg 1.000000e+04 train accuracy: 0.285592 val accuracy: 0.296000\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.162653 val accuracy: 0.153000\n",
      "lr 5.000000e-06 reg 1.000000e+05 train accuracy: 0.111959 val accuracy: 0.108000\n",
      "lr 5.000000e-06 reg 5.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.404000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [1e-8, 5e-8, 1e-7, 5e-7, 1e-6, 5e-6]\n",
    "regularization_strengths = [5e2, 1e3, 5e3, 1e4, 5e4, 1e5, 5e5]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                      num_iters=1500, batch_size=200, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        tr_acc = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        results[(lr, rs)] = (tr_acc, val_acc)\n",
    "        \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.389000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "True\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n",
    "If the weights are pre-trained such that the score of the target label for the new data point is above the threshold value, this data point will not change the overall loss, because per-datapoint loss will be zero:\n",
    "\n",
    "$$L_i^\\text{Hinge} = \\sum_{j\\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta)$$\n",
    "\n",
    "On the other hand, softmax loss is defined in a way such that any new data point will change the loss function because softmax wants to improve loss for any score, no matter how high or close to the target the prediction is:\n",
    "\n",
    "$$L_i^\\text{Softmax} = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
